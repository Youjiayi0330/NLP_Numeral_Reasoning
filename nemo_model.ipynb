{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24213,"status":"ok","timestamp":1714372020249,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"},"user_tz":240},"id":"5GLcrRSOXOqK","outputId":"49919c6c-0c56-41f0-db18-2e5ebb4e918b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import json\n","import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import BertTokenizer, BertModel\n","import tensorflow as tf\n","import keras\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"a12hAhgeXOqN"},"outputs":[],"source":["#####\n","# Global variables\n","#####\n","\n","# Check if CUDA can be used to speed up training/reasoning\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# CE\n","# Load BERT-large tokenizer and BERT-Large model\n","tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n","bert_model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext-large').to(device)  # Make sure the model is on the correct device\n","\n","# Define BiGRU layer for CE\n","hidden_size = 128  # For BERT-Large，the hidden_size should be 1024\n","bigru_layer = nn.GRU(input_size=1024, hidden_size=hidden_size, bidirectional=True, batch_first=True).to(device)\n","\n","# NE\n","# Character-to-index mapping\n","char_to_index = {str(i): i for i in range(10)}\n","char_to_index['+'] = 10\n","char_to_index['-'] = 11\n","char_to_index['e'] = 12\n","char_to_index['.'] = 13\n","\n","# Maximum numeric length and character dimension\n","max_num_length = 13\n","char_dim = 14\n","\n","# Initialize BiGRU for NE\n","input_size_NE = char_dim\n","hidden_size = 128\n","bigru_model = nn.GRU(input_size=input_size_NE, hidden_size=hidden_size, bidirectional=True, batch_first=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xeD_Nq4RXOqO","executionInfo":{"status":"ok","timestamp":1714372039256,"user_tz":240,"elapsed":6,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"outputs":[],"source":["# Define a function to encode text using BERT and BiGRU\n","def encode_with_ce(texts):\n","    # Encode the texts\n","    encoded_input = tokenizer(texts, return_tensors='pt',padding='max_length', truncation=True, max_length=512,\n","                              add_special_tokens=True)\n","\n","    # Make sure the input is also on the correct device\n","    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","    # Get the embedding using BERT-Large\n","    with torch.no_grad():\n","        output = bert_model(**encoded_input)\n","\n","    # The BERT model outputs a tuple, and we are interested in the first element - the hidden state\n","    embeddings = output.last_hidden_state\n","\n","    # Pass the embed to BiGRU\n","    bigru_output, _ = bigru_layer(embeddings)\n","\n","    last_output = bigru_output[:, -1, :]\n","\n","    return last_output"]},{"cell_type":"code","source":["def change_str_number_to_e(number):\n","        number = number.replace(',', '')\n","        number = float(number)\n","        formatted_number = f\"{number:e}\"\n","        # print(f'formatted_number: {formatted_number}')\n","        return formatted_number"],"metadata":{"id":"7TvimnYpp9kh","executionInfo":{"status":"ok","timestamp":1714372088374,"user_tz":240,"elapsed":151,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(change_str_number_to_e('10.1'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3CkB11kiDgq","executionInfo":{"status":"ok","timestamp":1714372840080,"user_tz":240,"elapsed":114,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}},"outputId":"e3ac028a-80ed-4976-840f-0fd06089ccdf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["1.010000e+01\n"]}]},{"cell_type":"code","execution_count":32,"metadata":{"id":"l76uYpJBXOqO","executionInfo":{"status":"ok","timestamp":1714340910649,"user_tz":240,"elapsed":243,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"outputs":[],"source":["# Convert answer's number into representations\n","def encode_and_process_number(number, max_num_length=13, char_dim=14, bigru_model=bigru_model):\n","    number = change_str_number_to_e(number)\n","    # Create an all-zero tensor\n","    encoded = torch.zeros(max_num_length, char_dim)\n","\n","    # Calculate the left fill amount\n","    padding_size = max_num_length - len(number)\n","\n","    # Fill encoding according to character\n","    for i, char in enumerate(number):\n","        if char in char_to_index:\n","            encoded[padding_size + i, char_to_index[char]] = 1\n","\n","    # Add batch dimension\n","    encoded = encoded.unsqueeze(0)  # Make the tensor shape [1, max_num_length, char_dim]\n","\n","    # Input the encoded tensor into BiGRU\n","    bigru_output, _ = bigru_model(encoded)\n","\n","    last_output = bigru_output[:, -1, :]\n","\n","    # Return the output of BiGRU\n","    return last_output"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"6TRWdBtbXOqP","executionInfo":{"status":"ok","timestamp":1714340914278,"user_tz":240,"elapsed":492,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"outputs":[],"source":["# Converts the correct answer index to a unique thermal encoding\n","def one_hot_encode(index, num_classes):\n","    encoding = [0] * num_classes\n","    encoding[index] = 1\n","    return encoding"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"k2CwcKybXOqP","executionInfo":{"status":"ok","timestamp":1714340916504,"user_tz":240,"elapsed":632,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"outputs":[],"source":["##########\n","# 1. Load the data\n","##########\n","\n","# Import the training data\n","with open('/content/drive/My Drive/ENLP/data/NQuAD_train_first_10k.json', 'r', encoding='utf-8') as file:\n","        data_train = json.load(file)\n","\n","# Import the testing data\n","with open('/content/drive/My Drive/ENLP/data/NQuAD_test_first_2k.json', 'r', encoding='utf-8') as file:\n","        data_test = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q0BkKYXOqQ"},"outputs":[],"source":["data_train[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0foCsE5XOqQ"},"outputs":[],"source":["data_train[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0-V6ebgXOqR"},"outputs":[],"source":["##########\n","# 2. Generate question representations for training data\n","##########\n","\n","# Prepare training data and one-hot labels lists\n","question_representations_train = []\n","one_hot_labels_train = []\n","batch_time = 0\n","\n","# Iterate each sample in training data\n","for sample in data_train[:10000]:\n","     # CE output list\n","        ce_output_list = []\n","     # NE output list\n","        ne_output_list = []\n","\n","     # 2.1 CE\n","        # Process question stem\n","        stem_result = encode_with_ce(sample['question_stem'])\n","        ce_output_list.append(stem_result)\n","\n","        # Process sentences_containing_the_numeral_in_answer_options\n","        for sentence_list in sample[\"sentences_containing_the_numeral_in_answer_options\"]:\n","\n","                if len(sentence_list) > 1:\n","                    combined_sentence = '[SEP]'.join(sentence.strip() for sentence in sentence_list)\n","                    result = encode_with_ce(combined_sentence)\n","                    ce_output_list.append(result)\n","                else:\n","                    result = encode_with_ce(sentence_list[0].strip())\n","                    ce_output_list.append(result)\n","\n","    # 2.2 NE\n","        numbers = sample[\"answer_options\"]\n","        for number in numbers:\n","            result = encode_and_process_number(number)\n","            ne_output_list.append(result)\n","        #             print(result.shape)\n","\n","    # 2.3 Concatenate\n","        all_output_list = ce_output_list + ne_output_list\n","        all_numpy_arrays_list = [tensor.detach().cpu().numpy() for tensor in all_output_list]\n","\n","        # Use tf. Keras. The layers. Concatenate to joining together all these tensor\n","        concat_layer = tf.keras.layers.Concatenate(axis=1)\n","        concatenated_tensors = concat_layer(all_numpy_arrays_list)\n","\n","        question_representations_train.append(concatenated_tensors)\n","\n","        # 2.5 Convert index of answer into one-hot vector\n","        correct_answer_index = [sample['ans']]\n","        # Converts the correct answer index to TensorFlow's uniquely thermal coded tensor\n","        one_hot_label = tf.one_hot(correct_answer_index, depth=4)\n","        #         print(one_hot_label)\n","        #         print(type(one_hot_label))\n","        one_hot_labels_train.append(one_hot_label)\n","        batch_time += 1\n","        print(f\"batch time: {batch_time}\")"]},{"cell_type":"code","source":["data_file_path = '/content/drive/My Drive/ENLP/train_data_10000.pt'\n","labels_file_path = '/content/drive/My Drive/ENLP/train_labels_10000.pt'\n","torch.save(question_representations_train, data_file_path)\n","torch.save(one_hot_labels_train, labels_file_path)"],"metadata":{"id":"W9vPkPrYI5BN","executionInfo":{"status":"ok","timestamp":1714348733089,"user_tz":240,"elapsed":5782,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["question_representations_train = torch.load(data_file_path)\n","one_hot_labels_train = torch.load(labels_file_path)\n","\n","features = tf.convert_to_tensor(question_representations_train)  # 假设已经是正确的格式\n","features = tf.squeeze(features, axis=1)\n","labels = tf.convert_to_tensor(one_hot_labels_train)\n","labels = tf.reshape(labels, [-1, 4])  # 确保标签是正确的形状\n","\n","##########\n","# 3. Make MLP model and put question representations and one-hot label list into MLP model\n","##########\n","\n","# Construct a MLP model\n","mlp = tf.keras.Sequential([\n","    tf.keras.layers.Dense(512, activation='relu', input_shape=(2304,)),  # 输入层节点数\n","    tf.keras.layers.Dropout(0.3),\n","    tf.keras.layers.Dense(256, activation='relu'),  # 第二层\n","    tf.keras.layers.Dropout(0.3),\n","    tf.keras.layers.Dense(4, activation='softmax')  # 输出层，假设有4个类别\n","\n","])\n","\n","# Compile model\n","optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95)\n","mlp.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train model\n","mlp.fit(features, labels, epochs=30, batch_size=256)"],"metadata":{"id":"G7kQmOYQhccW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8OqoiakXOqS"},"outputs":[],"source":["#####/\n","# 4. Prepare testing data and make predictions\n","#####\n","\n","# Prepare training data and one-hot labels lists\n","question_representations_test = []\n","one_hot_labels_test = []\n","batch_test_time = 0\n","\n","# Iterate each sample in training data\n","for sample in data_test[:2000]:\n","     # CE output list\n","        ce_output_list = []\n","     # NE output list\n","        ne_output_list = []\n","\n","     # 2.1 CE\n","        # Process question stem\n","        stem_result = encode_with_ce(sample['question_stem'])\n","        ce_output_list.append(stem_result)\n","\n","        # Process sentences_containing_the_numeral_in_answer_options\n","        for sentence_list in sample[\"sentences_containing_the_numeral_in_answer_options\"]:\n","                if len(sentence_list) > 1:\n","                    combined_sentence = '[SEP]'.join(sentence.strip() for sentence in sentence_list)\n","                    result = encode_with_ce(combined_sentence)\n","                    ce_output_list.append(result)\n","                else:\n","                    result = encode_with_ce(sentence_list[0].strip())\n","                    ce_output_list.append(result)\n","\n","    # 2.2 NE\n","        numbers = sample[\"answer_options\"]\n","        for number in numbers:\n","            result = encode_and_process_number(number)\n","            ne_output_list.append(result)\n","        #             print(result.shape)\n","\n","    # 2.3 Concatenate\n","        all_output_list = ce_output_list + ne_output_list\n","        all_numpy_arrays_list = [tensor.detach().cpu().numpy() for tensor in all_output_list]\n","\n","        # Use tf. Keras. The layers. Concatenate to joining together all these tensor\n","        concat_layer = tf.keras.layers.Concatenate(axis=1)\n","        concatenated_tensors = concat_layer(all_numpy_arrays_list)\n","\n","        # Add pooled tensor to question_representations\n","        question_representations_test.append(concatenated_tensors)\n","\n","        # 2.5 Convert index of answer into one-hot vector\n","        correct_answer_index = [sample['ans']]\n","        # Converts the correct answer index to TensorFlow's uniquely thermal coded tensor\n","        one_hot_label = tf.one_hot(correct_answer_index, depth=4)\n","        #         print(one_hot_label)\n","        #         print(type(one_hot_label))\n","        one_hot_labels_test.append(one_hot_label)\n","\n","        batch_test_time += 1\n","        print(f\"batch_test_time: {batch_test_time}\")\n"]},{"cell_type":"code","source":["test_data_file_path = '/content/drive/My Drive/ENLP/test_data_2000.pt'\n","test_labels_file_path = '/content/drive/My Drive/ENLP/test_labels_2000.pt'\n","torch.save(question_representations_test, test_data_file_path)\n","torch.save(one_hot_labels_test, test_labels_file_path)"],"metadata":{"id":"MI9F8OD8JnvU","executionInfo":{"status":"ok","timestamp":1714350730680,"user_tz":240,"elapsed":1258,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["question_representations_test = torch.load(test_data_file_path)\n","one_hot_labels_test = torch.load(test_labels_file_path)\n","\n","features_test = tf.convert_to_tensor(question_representations_test)  # 假设已经是正确的格式\n","features_test = tf.squeeze(features_test, axis=1)\n","labels_test = tf.convert_to_tensor(one_hot_labels_test)\n","labels_test = tf.reshape(labels_test, [-1, 4])  # 确保标签是正确的形状\n","\n","# Evaluate the model\n","# Evaluate the model using the test set data question_representations_test and one_hot_labels_test\n","loss, accuracy = mlp.evaluate(features_test, labels_test)\n","\n","print(f\"Loss: {loss}\")\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Nezuua0hY2s","executionInfo":{"status":"ok","timestamp":1714356302232,"user_tz":240,"elapsed":2061,"user":{"displayName":"JIAYI YOU","userId":"16220852766548676387"}},"outputId":"3dba48ce-bf2a-4df9-acbb-089d943bc0ce"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["63/63 [==============================] - 0s 4ms/step - loss: 1.2825 - accuracy: 0.4140\n","Loss: 1.2825226783752441\n","Accuracy: 0.414000004529953\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4812581,"sourceId":8140049,"sourceType":"datasetVersion"},{"datasetId":4806245,"sourceId":8131539,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}