{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lJvYxb66B5o8"},"outputs":[],"source":["!pip install datasets transformers\n","! pip install -U accelerate\n","! pip install -U transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCipZ4aRC2Wu","outputId":"38d45e22-db02-49e2-da94-99e284a6b7f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"pQOaBZBOEFzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","from transformers import AutoTokenizer\n","import json"],"metadata":{"id":"RlLuEiblD_5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"hfl/chinese-bert-wwm-ext\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"],"metadata":{"id":"yLmtYZMNEPwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##########\n","# 1. Load the data\n","##########\n","# Import the training data\n","with open('/content/drive/My Drive/NQuAD/NQuAD_train_first_10k_saved.json', 'r', encoding='utf-8') as file:\n","        data_train = json.load(file)\n","\n","# Import the testing data\n","with open('/content/drive/My Drive/NQuAD/NQuAD_test_first_2k_saved.json', 'r', encoding='utf-8') as file:\n","        data_test = json.load(file)"],"metadata":{"id":"BpT5KJUTD8nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train[0]"],"metadata":{"id":"fZMHRdWFcCgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_test[0]"],"metadata":{"id":"hFwIPu05g09U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict, ClassLabel, Features, Value\n","\n","# 定义数据集特征，为每个answer提供单独的字段\n","features = Features({\n","    'context': Value('string'),\n","    'answer0': Value('string'),\n","    'answer1': Value('string'),\n","    'answer2': Value('string'),\n","    'answer3': Value('string'),\n","    'label': ClassLabel(num_classes=4, names=['0', '1', '2', '3'])\n","})\n","\n","# 准备数据\n","prepared_train_data = {\n","    'context': [],\n","    'answer0': [],\n","    'answer1': [],\n","    'answer2': [],\n","    'answer3': [],\n","    'label': []\n","}\n","\n","prepared_test_data = {\n","    'context': [],\n","    'answer0': [],\n","    'answer1': [],\n","    'answer2': [],\n","    'answer3': [],\n","    'label': []\n","}\n","\n","for item in data_train:\n","    prepared_train_data['context'].append(item['context'].strip())\n","    prepared_train_data['label'].append(item['ans'])\n","    for i in range(4):\n","        prepared_train_data[f'answer{i}'].append(item[f'answer{i}'].strip())\n","\n","\n","for item in data_test:\n","    prepared_test_data['context'].append(item['context'].strip())\n","    prepared_test_data['label'].append(item['ans'])\n","    for i in range(4):\n","        prepared_test_data[f'answer{i}'].append(item[f'answer{i}'].strip())\n","\n","# 创建单个 Dataset\n","train_data = Dataset.from_dict(prepared_train_data, features=features)\n","test_data = Dataset.from_dict(prepared_test_data, features=features)"],"metadata":{"id":"n6LAyLRVgPJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets = DatasetDict({\n","    'train': train_data,\n","    'test': test_data\n","})"],"metadata":{"id":"oVjK9_VYho2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets['train'][0]"],"metadata":{"id":"2NJjADqhhwL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets['train'][1]"],"metadata":{"id":"9nfK_mx3tJ0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=3):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","\n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))"],"metadata":{"id":"bUVCH4oajdrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_random_elements(datasets[\"train\"])"],"metadata":{"id":"C102vCQrjf6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_one(example):\n","    print(f\"Context: {example['context']}\")\n","    print(f\"  A - {example['answer0']}\")\n","    print(f\"  B - {example['answer1']}\")\n","    print(f\"  C - {example['answer2']}\")\n","    print(f\"  D - {example['answer3']}\")\n","    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")\n","\n","show_one(datasets[\"train\"][0])\n","\n","# Context: 宏達電（2498）昨(21)日股東會上，董事長王雪紅應允小股東請求，自掏腰包贈送股東會議程結束前完成報到程序且人在會場的小股東（不含公司內部股東），每人一台宏達電旗艦手機新HTC One 32G手機、單機市價約2.19萬元，創下台灣股東會史上單價最高的贈禮。同時，宏達電董監改選，前台積電（2330）總經理蔡力行當選新任董事，被外界認為是跟台積電聯手抗韓，針對三星電子的科技競爭而進行的結盟。據統計，當日符合條件股東數約100多位，依單機2.19萬元計算，王雪紅大手筆送給在場關心宏達電營運的小股東們逾200多萬元大禮。經過各家媒體近兩日報導，「宏達電」、「HTC」、「新HTC One」在網路上討論的熱烈程度又升高了，達到品牌與知名度的提升效果，宏達電行銷功力可謂更上層樓。宏達電今年的股東會也全面進行董監事改選，當選董事名單包括王雪紅、陳文琦、卓火土、蔡力行、David Bruce Yoffie；獨立董事為林振國、Josef Felder；新任監察人為威智投資(股)公司、朱黃傑。除了蔡力行是新赴任的宏達電董事外，其餘本次當選董事、監察人都為續任。蔡力行是台積電（2330）前總經理、現任台積電太陽能與固態照明董事長；因此外界也多解讀，在台積電董事長張忠謀公開多次盛讚宏達電手機產品，並倡議聯發科（2454）、鴻海（2317）、台積電與宏達電在各自的半導體IC、面板、晶圓產業、消費性手機領域一同對付三星電子；蔡力行昨日獲選出任宏達電董事一職，可謂是台灣科技廠聯手抗韓的最新發展。\n","#   A - 王雪紅贈股東100G新One，創台股東會單價最高贈品\n","#   B - 王雪紅贈股東2.19G新One，創台股東會單價最高贈品\n","#   C - 王雪紅贈股東21G新One，創台股東會單價最高贈品\n","#   D - 王雪紅贈股東32G新One，創台股東會單價最高贈品\n","\n","# Ground truth: option D"],"metadata":{"id":"BQlB_UorkTRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","model_checkpoint = \"hfl/chinese-bert-wwm-ext\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, padding=True, truncation=True, max_length=512)"],"metadata":{"id":"fEahA3wRl0u8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(datasets['train'][0]['answer0'])"],"metadata":{"id":"4CvnkVLerhJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans_names = ['answer0', 'answer1', 'answer2', 'answer3']\n","\n","def preprocess_function(examples):\n","\n","    # 预处理输入tokenizer的输入\n","    # Repeat each first sentence four times to go with the four possibilities of second sentences.\n","    first_sentences = [[context] * 4 for context in examples[\"context\"]]# 构造和备选项个数相同的问题句，也是tokenizer的第一个句子\n","\n","    # 构造所有的second sentences，每个second sentence是一个备选答案\n","    second_sentences = [\n","        [examples[ans][i] for ans in ans_names] for i in range(len(examples[\"context\"]))\n","    ]\n","\n","    # # Flatten everything\n","    first_sentences = sum(first_sentences, []) # 合并成一个列表方便tokenizer一次性处理\n","    second_sentences = sum(second_sentences, [])# 合并成一个列表方便tokenizer一次性处理\n","\n","    # Tokenize\n","    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True,  max_length=512)\n","\n","    # Un-flatten\n","    # 转化成每个样本（一个样本中包括了四个k=[问题1,问题1,问题1,问题1],v=[备选项1,备选项2,备选项3,备选项4]）\n","\n","    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"],"metadata":{"id":"3zw1qEIWmBw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples = datasets[\"train\"][:2]\n","features = preprocess_function(examples)"],"metadata":{"id":"Jm9gzoFqn1Q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 0\n","[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]"],"metadata":{"id":"Dn4AY-MVxPzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = datasets.map(preprocess_function, batched=True)"],"metadata":{"id":"0S1dSgN69zQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForMultipleChoice\n","model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)"],"metadata":{"id":"gaC7mijm-UaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import accelerate\n","print(accelerate.__version__)"],"metadata":{"id":"7ic5kWWO_nou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["task='ner'\n","batch_size = 5\n","\n","from transformers import  TrainingArguments"],"metadata":{"id":"egNTBi_u_1QD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    \"my_model_output\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")"],"metadata":{"id":"2tWWxPLI_3RF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from typing import Optional, Union\n","import torch"],"metadata":{"id":"bTmNzY41Bodc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs for multiple choice received.\n","    \"\"\"\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features):\n","        #features:[{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_},{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_}]\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        labels = [feature.pop(label_name) for feature in features] #将label单独弹出，features:[{'attention_mask':[[],[],...],'input_ids':[[],[],...]},{'attention_mask':[[],[],...],'input_ids':[[],[],...]}]\n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","\n","        #feature:{'attention_mask':[[],[],...],'input_ids':[[],[],...]}\n","        #flattened_features:[[{'attention_mask':[],'input_ids':[]},{},{},{}],[]....]\n","        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n","        #flattened_features:[{'attention_mask':[],'input_ids':[]},{},{},{},{}....]\n","        flattened_features = sum(flattened_features, [])\n","\n","        # batch: {'attention_mask':[[],[],[],[],[],[],...],'input_ids':[[],[],[],[],[],[],...]}\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # Un-flatten\n","        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...]}\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # Add back labels\n","        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...],'label':[]}\n","        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch"],"metadata":{"id":"dn3cKDiFBuvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n","features = [{k: v for k, v in tokenized_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n","batch = DataCollatorForMultipleChoice(tokenizer)(features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ch6XsNLWCf2U","outputId":"38b1f60e-7afb-41d2-9d23-25c52c7cbd8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}]},{"cell_type":"code","source":["features[0]"],"metadata":{"id":"STZN3CzpF6Cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(4)]"],"metadata":{"id":"v5B7lZ1yGO_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_one(datasets[\"train\"][8])\n","#    Context: Someone walks over to the radio.\n","#      A - Someone hands her another phone.\n","#      B - Someone takes the drink, then holds it.\n","#      C - Someone looks off then looks at someone.\n","#      D - Someone stares blearily down at the floor.\n","#\n","#    Ground truth: option D"],"metadata":{"id":"ND6nm0mzHBP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_metric\n","def compute_metrics(eval_predictions):\n","    predictions, label_ids = eval_predictions\n","    preds = np.argmax(predictions, axis=1)\n","    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"],"metadata":{"id":"Ur2GQUxbHKcY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import  Trainer\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer),\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"hiMavCwfHmH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!export TF_FORCE_GPU_ALLOW_GROWTH=true"],"metadata":{"id":"YmpwxOdYKF06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"sZe-ciE1Hrzq"},"execution_count":null,"outputs":[]}]}